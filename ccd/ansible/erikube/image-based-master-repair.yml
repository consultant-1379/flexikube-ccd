---
- name: Worker to repair
  hosts: "etcd-{{ master_to_fix.split('-')[1] }}"
  tasks:
    - debug:
        var: master_to_fix

# ETCD and Masters are splitted because there is ssh bug that causes failures
# if two ssh connections are made to same node simultaneously
- name: Gather facts as non root from etcds
  hosts: etcd
  gather_facts: yes

- name: Gather facts as non root from masters
  hosts: master
  gather_facts: yes

- name: Set ECCD default values
  hosts: master
  roles:
    - erikube-defaults

- name: Set ECCD default values
  hosts: etcd
  roles:
    - erikube-defaults

- name: Remove broken ETCD node
  hosts: "{{ 'etcd-' + (( master_to_fix.split('-')[1] | int + 1) % 3 ) | string }}"
  become: yes
  any_errors_fatal: true
  vars:
    etcd_hostname: "{{ master_to_fix }}"
  tasks:
    - include_role:
        name: etcd
        tasks_from: remove_member

- name: Deploy repaired ETCD node
  hosts: "etcd-{{ master_to_fix.split('-')[1] }}"
  become: yes
  any_errors_fatal: true
  vars:
    etcd_ca_host: "{{ groups.etcd[((master_to_fix.split('-')[1] | int + 1) % 3)] }}"
    etcd_peers: "{{ groups.etcd }}"
    etcd_initial_cluster_state: "existing"
    working_etcd: "{{ groups.etcd[((master_to_fix.split('-')[1] | int + 1) % 3)] }}"
  tasks:
    - include_role:
        name: etcd
        tasks_from: ca-deploy
    - include_role:
        name: etcd
        tasks_from: add_member
    - include_role:
        name: etcd
        tasks_from: deploy
    - include_role:
        name: etcd
        tasks_from: check-health

- name: Set common etcd related vars
  hosts: "{{ master_to_fix }}"
  tasks:
    - set_fact:
        etcd_peers: "{{ groups.etcd }}"
        etcd_ca_host: "{{ groups.etcd[((master_to_fix.split('-')[1] | int + 1) % 3)] }}"

- name: Deploy new Kubernetes masters
  hosts: "{{ master_to_fix }}"
  serial: 1
  roles:
    - role: kube-master
      vars:
        alternative_first_master:
          "{{ 'master-' + (( master_to_fix.split('-')[1] | int + 1) % 3 ) | string }}"

- name: Update Kubernetes API VIP
  hosts: master
  become: yes
  tasks:
  - include_role:
      name: kube-api-vip
    vars:
      keepalived_master: "{{ groups.master[0] }}"
      keepalived_vip: "{{ kube_apiserver_ip }}"
      kube_lb_upstream_servers: "{{ groups.master }}"
      keepalived_unicast_src_ip: "{{ k8s_ip_address }}"
      keepalived_unicast_peers: "{{ groups.master | difference([inventory_hostname]) }}"
    when:
    - kube_api_vip_enabled | default(false) | bool
